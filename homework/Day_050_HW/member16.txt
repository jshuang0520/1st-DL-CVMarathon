深度學習與電腦視覺期末專題_浣熊與袋鼠辨識模型(黃士繽)
2020/03/04
一、專題摘要 (解釋實作與說明需要解決的問題，限300~500字。)
期末專題主題   浣熊與袋鼠辨識模型
期末專題基本目標
                建立一個模型同時能辨識出浣熊(raccoon)與袋鼠(kangaroo)的類別與位置。

二、實作方法介紹 (介紹使用的程式碼、模組，並附上實作過程與結果的截圖，需圖文並茂。)
1.使用的程式碼介紹

1.1 前置準備 ( 安裝 keras 、 下載 keras-yolo3及訓練資料集下載)

## 安裝 keras 及 下載 keras-yolo3
!pip install keras==2.2.4                                 # 安裝 keras 2.2.4 的版本
!git clone https://github.com/qqwweee/keras-yolo3         # 使用 keras-yolo3 
%cd keras-yolo3

# 訓練資料集下載
!git clone https://github.com/experiencor/kangaroo.git          # kangaroo 圖片集
!git clone https://github.com/experiencor/raccoon_dataset.git   # raccoon 圖片集

# 下載測試資料
!mkdir test
!wget 'https://dehayf5mhw1h7.cloudfront.net/wp-content/uploads/sites/726/2017/08/03065555/Kangacoon-1024x768.jpg' -O 'test/Kan_Raccoon1.jpg'

!mkdir video
!wget -c 'https://cvdl.cupoy.com/HomeworkAction.do?op=getHomeworkFileContent&hwid=D49&filepath=Raccoon.mp4' -O 'video/Raccoon.mp4'      # kangaroo 測試影片
!wget -c 'https://cvdl.cupoy.com/HomeworkAction.do?op=getHomeworkFileContent&hwid=D49&filepath=Kangaroo.mp4' -O 'video/Kangaroo.mp4'    # kangaroo 測試影片
## 安裝 keras 及 下載 keras-yolo3
!pip install keras==2.2.4                                 # 安裝 keras 2.2.4 的版本
!git clone https://github.com/qqwweee/keras-yolo3         # 使用 keras-yolo3 
%cd keras-yolo3

# 訓練資料集下載
!git clone https://github.com/experiencor/kangaroo.git          # kangaroo 圖片集
!git clone https://github.com/experiencor/raccoon_dataset.git   # raccoon 圖片集

# 下載測試資料
!mkdir test
!wget 'https://dehayf5mhw1h7.cloudfront.net/wp-content/uploads/sites/726/2017/08/03065555/Kangacoon-1024x768.jpg' -O 'test/Kan_Raccoon1.jpg'

!mkdir video
!wget -c 'https://cvdl.cupoy.com/HomeworkAction.do?op=getHomeworkFileContent&hwid=D49&filepath=Raccoon.mp4' -O 'video/Raccoon.mp4'      # kangaroo 測試影片
!wget -c 'https://cvdl.cupoy.com/HomeworkAction.do?op=getHomeworkFileContent&hwid=D49&filepath=Kangaroo.mp4' -O 'video/Kangaroo.mp4'    # kangaroo 測試影片

1.2 資料集影像處理 (解析 xml 資料)

    1.2.1  使用xml.etree.ElementTree 解析 xml 資料到 animals_train.txt

    1.2.2 自定義convert_annotation

cls_name 分類至 cls_index (0,1)
bndbox 顯示影像方框
   1.2.3 設定圖片檔案位置，列出影像位置  images_dir = ['kangaroo/images', 'raccoon_dataset/images'] 

import xml.etree.ElementTree as ET                 # 載入能夠 Parser xml 文件的 library

if not os.path.exists('animals_train.txt'):

    # 資料集格式轉換   
    def convert_annotation(xml_path, list_file):
        f = open(xml_path)
        tree = ET.parse(f)
        root = tree.getroot()
        
        for obj in tree.iter('object'):
            diff = int(obj.find('difficult').text)
            cls_name = obj.find('name').text
            
            if cls_name not in classes or diff == 1:
                continue
                
            cls_index = classes.index(cls_name)
            bndbox = obj.find('bndbox')
            
            b = (int(bndbox.find('xmin').text), 
                int(bndbox.find('ymin').text), 
                int(bndbox.find('xmax').text), 
                int(bndbox.find('ymax').text))

            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_index))        
        f.close()
        
        
    # 讀取資料位置
    images_dir = ['kangaroo/images', 'raccoon_dataset/images']
    classes = ["kangaroo", "raccoon"]
    sets = ['train', 'val']
    
    for s in sets:
        list_file = open(f'animals_{s}.txt', 'w')
        
        for i, annotation_path in enumerate(['kangaroo/annots', 'raccoon_dataset/annotations']):
            annots = os.listdir(annotation_path)
            annots.sort()
            annot_size = len(annots)
            split_num = int(np.round(annot_size * 0.8))
        
            if s == 'train':
                data = annots[:split_num]
            else:
                data = annots[split_num:]
                
            for d in data:
                name = d.split('.')[0]
                list_file.write(os.path.join(images_dir[i], f'{name}.jpg'))
                convert_annotation(os.path.join(annotation_path, d), list_file)
                list_file.write('\n')
        
        list_file.close()
import xml.etree.ElementTree as ET                 # 載入能夠 Parser xml 文件的 library

if not os.path.exists('animals_train.txt'):

    # 資料集格式轉換   
    def convert_annotation(xml_path, list_file):
        f = open(xml_path)
        tree = ET.parse(f)
        root = tree.getroot()
        
        for obj in tree.iter('object'):
            diff = int(obj.find('difficult').text)
            cls_name = obj.find('name').text
            
            if cls_name not in classes or diff == 1:
                continue
                
            cls_index = classes.index(cls_name)
            bndbox = obj.find('bndbox')
            
            b = (int(bndbox.find('xmin').text), 
                int(bndbox.find('ymin').text), 
                int(bndbox.find('xmax').text), 
                int(bndbox.find('ymax').text))

            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_index))        
        f.close()
        
        
    # 讀取資料位置
    images_dir = ['kangaroo/images', 'raccoon_dataset/images']
    classes = ["kangaroo", "raccoon"]
    sets = ['train', 'val']
    
    for s in sets:
        list_file = open(f'animals_{s}.txt', 'w')
        
        for i, annotation_path in enumerate(['kangaroo/annots', 'raccoon_dataset/annotations']):
            annots = os.listdir(annotation_path)
            annots.sort()
            annot_size = len(annots)
            split_num = int(np.round(annot_size * 0.8))
        
            if s == 'train':
                data = annots[:split_num]
            else:
                data = annots[split_num:]
                
            for d in data:
                name = d.split('.')[0]
                list_file.write(os.path.join(images_dir[i], f'{name}.jpg'))
                convert_annotation(os.path.join(annotation_path, d), list_file)
                list_file.write('\n')
        
        list_file.close()
1.3 設定訓練資料來源位置，分配 training data 以及 validation 比例

# 設定訓練資料來源位置， 分配training data 以及 validation 比例

annotation_path = 'animals_train.txt'                                  # 轉換好格式的標註檔案
log_dir = 'logs/000/'                                                  # 訓練好的模型儲存的路徑
classes_path = 'model_data/animal_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)


# 分為 training 以及 validation
val_split = 0.25
with open(annotation_path) as f:
    train_lines = f.readlines()

with open('animals_val.txt') as f:
    val_lines = f.readlines()

np.random.seed(10101)
np.random.shuffle(train_lines)
np.random.seed(None)
num_val = len(val_lines)
num_train = len(train_lines)
# 設定訓練資料來源位置， 分配training data 以及 validation 比例

annotation_path = 'animals_train.txt'                                  # 轉換好格式的標註檔案
log_dir = 'logs/000/'                                                  # 訓練好的模型儲存的路徑
classes_path = 'model_data/animal_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)


# 分為 training 以及 validation
val_split = 0.25
with open(annotation_path) as f:
    train_lines = f.readlines()

with open('animals_val.txt') as f:
    val_lines = f.readlines()

np.random.seed(10101)
np.random.shuffle(train_lines)
np.random.seed(None)
num_val = len(val_lines)
num_train = len(train_lines)
1.4 模型設定與訓練

    1.4.1  載入 yolov3 Keras模型與權重

#  載入 yolov3 Keras模型與權重

if not os.path.exists("model_data/yolo.h5"):                    # 載入 yolov3 Keras模型 (hdf5存储格式)
    print("Model doesn't exist, downloading...")
    os.system("wget https://pjreddie.com/media/files/yolov3.weights")
    print("Converting yolov3.weights to yolo.h5...")
    os.system("python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5")
else:
    print("Model exist")

if not os.path.exists("model_data/yolo_weights.h5"):             # 載入 yolov3 的網路權重
    print("Converting pretrained YOLOv3 weights for training")
    os.system("python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5") 
else:
    print("Pretrained weights exists")
#  載入 yolov3 Keras模型與權重

if not os.path.exists("model_data/yolo.h5"):                    # 載入 yolov3 Keras模型 (hdf5存储格式)
    print("Model doesn't exist, downloading...")
    os.system("wget https://pjreddie.com/media/files/yolov3.weights")
    print("Converting yolov3.weights to yolo.h5...")
    os.system("python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5")
else:
    print("Model exist")

if not os.path.exists("model_data/yolo_weights.h5"):             # 載入 yolov3 的網路權重
    print("Converting pretrained YOLOv3 weights for training")
    os.system("python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5") 
else:
    print("Pretrained weights exists")
        1.4.2  模型參數設定 (logging, checkpoint, reduce_lr, early_stopping..)

# 模型參數設定 ，設定logging, checkpoint, reduce_lr, early_stopping

input_shape = (416,416)                                                 # multiple of 32, hw

is_tiny_version = len(anchors)==6                                       # default setting
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/tiny_yolo.h5')
else:
    model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/yolo_weights.h5')   # make sure you know what you freeze
    # model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='logs/000/trained_weights_final.h5')
                        
logging = TensorBoard(log_dir=log_dir)

checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.1, 
                              patience=10, 
                              verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', 
                               min_delta=0.001, 
                               patience=20, 
                               verbose=1)
# 模型參數設定 ，設定logging, checkpoint, reduce_lr, early_stopping

input_shape = (416,416)                                                 # multiple of 32, hw

is_tiny_version = len(anchors)==6                                       # default setting
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/tiny_yolo.h5')
else:
    model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/yolo_weights.h5')   # make sure you know what you freeze
    # model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='logs/000/trained_weights_final.h5')
                        
logging = TensorBoard(log_dir=log_dir)

checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.1, 
                              patience=10, 
                              verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', 
                               min_delta=0.001, 
                               patience=20, 
                               verbose=1)
    1.4.3 自定義圖像擴增 data_generator

# 自定義圖像擴增 data_generator
def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, is_train=True):
    '''data generator for fit_generator'''
    n = len(annotation_lines)
    i = 0
    while True:
        image_data = []
        box_data = []
        for b in range(batch_size):
            if i==0 and is_train:
                np.random.shuffle(annotation_lines)
            image, box = get_random_data(annotation_lines[i], input_shape, random=is_train)
            image_data.append(image)
            box_data.append(box)
            i = (i+1) % n
        image_data = np.array(image_data)
        box_data = np.array(box_data)
        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)
        yield [image_data, *y_true], np.zeros(batch_size)
# 自定義圖像擴增 data_generator
def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, is_train=True):
    '''data generator for fit_generator'''
    n = len(annotation_lines)
    i = 0
    while True:
        image_data = []
        box_data = []
        for b in range(batch_size):
            if i==0 and is_train:
                np.random.shuffle(annotation_lines)
            image, box = get_random_data(annotation_lines[i], input_shape, random=is_train)
            image_data.append(image)
            box_data.append(box)
            i = (i+1) % n
        image_data = np.array(image_data)
        box_data = np.array(box_data)
        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)
        yield [image_data, *y_true], np.zeros(batch_size)
    1.4.4  第一階段訓練: 先 freeze YOLO (除了 output layer) 

                                以 backbone Model (yolo.h5) 及預設的權重 來 train 模型

# 第一階段訓練: 先 freeze YOLO (除了 output layer) 以 backbone Model (yolo.h5) 及預設的權重 來 train 模型

if True:
    batch_size=5
    model.compile(optimizer=Adam(lr=1e-3), loss={'yolo_loss': lambda y_true, y_pred: y_pred})           # use custom yolo_loss Lambda layer.
    print(f'第一階段訓練 : Train on {num_train} samples, val on {num_val} samples, with batch size {batch_size}.')

    history1 = model.fit_generator(data_generator(train_lines, batch_size, input_shape, anchors, num_classes, False),   # 使用原本的資料集訓練
            steps_per_epoch=max(1, num_train//batch_size),
            epochs=15,
            validation_data=data_generator(val_lines, batch_size, input_shape, anchors, num_classes, False),
            validation_steps=max(1, num_val//batch_size),
            initial_epoch=0,
            callbacks=[logging, checkpoint, reduce_lr, early_stopping])
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')
# 第一階段訓練: 先 freeze YOLO (除了 output layer) 以 backbone Model (yolo.h5) 及預設的權重 來 train 模型

if True:
    batch_size=5
    model.compile(optimizer=Adam(lr=1e-3), loss={'yolo_loss': lambda y_true, y_pred: y_pred})           # use custom yolo_loss Lambda layer.
    print(f'第一階段訓練 : Train on {num_train} samples, val on {num_val} samples, with batch size {batch_size}.')

    history1 = model.fit_generator(data_generator(train_lines, batch_size, input_shape, anchors, num_classes, False),   # 使用原本的資料集訓練
            steps_per_epoch=max(1, num_train//batch_size),
            epochs=15,
            validation_data=data_generator(val_lines, batch_size, input_shape, anchors, num_classes, False),
            validation_steps=max(1, num_val//batch_size),
            initial_epoch=0,
            callbacks=[logging, checkpoint, reduce_lr, early_stopping])
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')

        1.4.5  第二階段訓練: 把所有 layer 都改為 trainable ，做 fine-tune 

# 第二階段訓練: 把所有 layer 都改為 trainable ，做 fine-tune

if True:
    print('\n 第二階段訓練 : Unfreeze all of the layers.')
    for i in range(len(model.layers)):                            # 把所有 layer 都改為 trainable
        model.layers[i].trainable = True

    batch_size=3                                                  # note that more GPU memory is required after unfreezing the body       
    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred})      # recompile to apply the change
    print(f'Train on {num_train} samples, val on {num_val} samples, with batch size {batch_size}.')

    history2 = model.fit_generator(data_generator(train_lines, batch_size, input_shape, anchors, num_classes, True),    # 將原本的資料集調變後訓練
        steps_per_epoch=max(1, num_train//batch_size),
        epochs=15,
        validation_data=data_generator(val_lines, batch_size, input_shape, anchors, num_classes, False),
        validation_steps=max(1, num_val//batch_size),
        initial_epoch=0,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
 
    model.save_weights(log_dir + 'trained_weights_final.h5')
# 第二階段訓練: 把所有 layer 都改為 trainable ，做 fine-tune

if True:
    print('\n 第二階段訓練 : Unfreeze all of the layers.')
    for i in range(len(model.layers)):                            # 把所有 layer 都改為 trainable
        model.layers[i].trainable = True

    batch_size=3                                                  # note that more GPU memory is required after unfreezing the body       
    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred})      # recompile to apply the change
    print(f'Train on {num_train} samples, val on {num_val} samples, with batch size {batch_size}.')

    history2 = model.fit_generator(data_generator(train_lines, batch_size, input_shape, anchors, num_classes, True),    # 將原本的資料集調變後訓練
        steps_per_epoch=max(1, num_train//batch_size),
        epochs=15,
        validation_data=data_generator(val_lines, batch_size, input_shape, anchors, num_classes, False),
        validation_steps=max(1, num_val//batch_size),
        initial_epoch=0,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
 
    model.save_weights(log_dir + 'trained_weights_final.h5')
        1.4.6  繪出 訓練過程 val_loss 與 loss 的變化 

# 繪出 訓練過程 val_loss 與 loss 的變化

import matplotlib.pyplot as plt

plt.figure()
plt.title('Learning rate- stage 1 ')  
plt.plot(history1.history["val_loss"], "-*" ,  label="val_loss")
plt.plot(history1.history["loss"], label="loss")
plt.legend(loc='upper right')

plt.figure()
plt.title('Learning rate- stage 2 ')  
plt.plot(history2.history["val_loss"], "-*" ,  label="val_loss")
plt.plot(history2.history["loss"], label="loss")
plt.legend(loc='upper right')
plt.xlabel("epochs")

# plt.savefig('history.png')
# model.summary()
# 繪出 訓練過程 val_loss 與 loss 的變化

import matplotlib.pyplot as plt

plt.figure()
plt.title('Learning rate- stage 1 ')  
plt.plot(history1.history["val_loss"], "-*" ,  label="val_loss")
plt.plot(history1.history["loss"], label="loss")
plt.legend(loc='upper right')

plt.figure()
plt.title('Learning rate- stage 2 ')  
plt.plot(history2.history["val_loss"], "-*" ,  label="val_loss")
plt.plot(history2.history["loss"], label="loss")
plt.legend(loc='upper right')
plt.xlabel("epochs")

# plt.savefig('history.png')
# model.summary()
1.5 模型評估

    1.5.1 picture detector

# 載入yolo模型 與 訓練後的權重

from yolo import YOLO
yolo_model = YOLO(model_path=log_dir + 'trained_weights_final.h5', classes_path=classes_path)

from PIL import Image
from IPython import display

for file in os.listdir('test'):
    image = Image.open(os.path.join('test/', file))      # 讀取範例圖片
    r_img = yolo_model.detect_image(image)               # 執行 yolo 檢測，將回傳的圖片儲存在 r_image 中
    display.display(r_img)                               # 顯示 r_image，可觀察到圖片上已畫上 yolov3 所檢測的 object
# 載入yolo模型 與 訓練後的權重

from yolo import YOLO
yolo_model = YOLO(model_path=log_dir + 'trained_weights_final.h5', classes_path=classes_path)

from PIL import Image
from IPython import display

for file in os.listdir('test'):
    image = Image.open(os.path.join('test/', file))      # 讀取範例圖片
    r_img = yolo_model.detect_image(image)               # 執行 yolo 檢測，將回傳的圖片儲存在 r_image 中
    display.display(r_img)                               # 顯示 r_image，可觀察到圖片上已畫上 yolov3 所檢測的 object
      1.5.2 video detector

                1.5.2.1 自訂義 video_detect

# 參照來源: https://github.com/gausschang/1st-DL-CVMarathon/blob/master/Day050_HomeWork.ipynb

def video_detect(video_path, output_path):
    import cv2
    import numpy as np
    from PIL import Image
    import timeit

    starttime = timeit.default_timer()
    cap = cv2.VideoCapture(video_path)

    # 使用 XVID 編碼
    fourcc = cv2.VideoWriter_fourcc(*'XVID')

    # 建立 VideoWriter 物件，輸出影片至 output.avi , FPS 值為 30.0，解析度為 1280*720
    out = cv2.VideoWriter(output_path, fourcc, 30.0, (1280, 720))

    k = 0
    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == True:
            # 寫入影格
            image = Image.fromarray(frame)
            image = np.array(yolo_model.detect_image(image))
            out.write(image)
            k += 1
        else:
            break
        
    # 釋放所有資源
    cap.release()
    out.release()
    print('共耗時:',round((timeit.default_timer() - starttime), 2), '秒，FPS:', round(k/(timeit.default_timer() - starttime), 2))
# 參照來源: https://github.com/gausschang/1st-DL-CVMarathon/blob/master/Day050_HomeWork.ipynb

def video_detect(video_path, output_path):
    import cv2
    import numpy as np
    from PIL import Image
    import timeit

    starttime = timeit.default_timer()
    cap = cv2.VideoCapture(video_path)

    # 使用 XVID 編碼
    fourcc = cv2.VideoWriter_fourcc(*'XVID')

    # 建立 VideoWriter 物件，輸出影片至 output.avi , FPS 值為 30.0，解析度為 1280*720
    out = cv2.VideoWriter(output_path, fourcc, 30.0, (1280, 720))

    k = 0
    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == True:
            # 寫入影格
            image = Image.fromarray(frame)
            image = np.array(yolo_model.detect_image(image))
            out.write(image)
            k += 1
        else:
            break
        
    # 釋放所有資源
    cap.release()
    out.release()
    print('共耗時:',round((timeit.default_timer() - starttime), 2), '秒，FPS:', round(k/(timeit.default_timer() - starttime), 2))
        1.5.2 影片偵測結果

# Kangaroo 影片
# video_path = 'video/Kangaroo.mp4'
# output_path = 'video/Kangaroo-yolo.mp4'
# video_detect(video_path=video_path, output_path=output_path)
# 共耗時: 132.33 秒，FPS: 14.16

# https://drive.google.com/open?id=1-rCII3zd5ilGpTxMTYB3FeO95emUJnih

# Raccoon 影片
# video_path = 'video/Raccoon.mp4'
# output_path = 'video/Raccoon-yolo.mp4'
# video_detect(video_path=video_path, output_path=output_path)    
# 共耗時: 127.07 秒，FPS: 14.16

# https://drive.google.com/open?id=1-p3wsj4MFrCZt_XN13Gq56momoSHd7sl
# Kangaroo 影片
# video_path = 'video/Kangaroo.mp4'
# output_path = 'video/Kangaroo-yolo.mp4'
# video_detect(video_path=video_path, output_path=output_path)
# 共耗時: 132.33 秒，FPS: 14.16

# https://drive.google.com/open?id=1-rCII3zd5ilGpTxMTYB3FeO95emUJnih

# Raccoon 影片
# video_path = 'video/Raccoon.mp4'
# output_path = 'video/Raccoon-yolo.mp4'
# video_detect(video_path=video_path, output_path=output_path)    
# 共耗時: 127.07 秒，FPS: 14.16

# https://drive.google.com/open?id=1-p3wsj4MFrCZt_XN13Gq56momoSHd7sl
2.使用的模組介紹

# 安裝相關套件 ，將 training 所需要的套件載入
import os
import subprocess
import numpy as np
import keras.backend as K
from keras.layers import Input, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from yolo3.utils import get_random_data
from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss
from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

import tensorflow as tf
# 安裝相關套件 ，將 training 所需要的套件載入
import os
import subprocess
import numpy as np
import keras.backend as K
from keras.layers import Input, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from yolo3.utils import get_random_data
from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss
from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

import tensorflow as tf

三、成果展示 (介紹成果的特點為何，並撰寫心得。)
1. 模型兩階段訓練結果。

        第一階段訓練，減少batch數，使用原圖檔訓練，可快速收斂。(batch = 5)

        第二階段訓練，使用小batch數(=3)，增加圖檔變形，可使loss有效降低。




2. 圖像辨識有足夠的能力!


3.影片辨識結果
   3.1 Kangaroo 影片  共耗時: 132.33 秒，FPS: 14.16

        # https://drive.google.com/open?id=1-rCII3zd5ilGpTxMTYB3FeO95emUJnih



    3.2 Raccoon 影片 共耗時: 127.07 秒，FPS: 14.16

        # https://drive.google.com/open?id=1-p3wsj4MFrCZt_XN13Gq56momoSHd7sl

四、結論 (總結本次專題的問題與結果)
    4.1 影片辨識能力明顯不足

        => 增加不同角度圖片，或增加訓練次數(epoch)



    4.2 此次專題未加入IoU 與mAP 指標....



五、期末專題作者資訊 (請附上作者資訊)
個人Github連結      

https://github.com/soldierHuang/1st-DL-CVMarathon/blob/master/Day049_050__HW_ipynb.ipynb

1st-DL-CVMarathon/Day049_050__HW_ipynb.ipynb at master · soldierHuang/1st-DL-CVMarathon · GitHub
Contribute to soldierHuang/1st-DL-CVMarathon development by creating an account on GitHub.



個人在百日馬拉松顯示名稱  黃士繽
分享
留言(1)
Mora
2020/03/05
能在短時間就能實作出來，在結論中有提到 IoU 相關的指標，如果還有時間，可以在撰寫程式碼，計算測試圖片中的 IoU的變化，和 mAP 的運用，就可以凸顯個人專題的獨到之處。