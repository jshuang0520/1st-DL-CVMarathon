深度學習與電腦視覺馬拉松期末專題_Gary
2020/05/11
一、專題摘要

1. 期末專題主題：是否正確配戴口罩的辨識模型

2. 期末專題基本目標︰

    · 可分辨口罩配戴狀況

    · 嘗試Transfer Learning中對全層Training100 epoch

      和先對output Layer進行50次Training再進行50次全層Training是否結果會較好





二、實作方法介紹：

1. 資料集：

    從以下的網址提供之口罩資料集:

    https://www.facebook.com/groups/Taiwan.AI.Group/?post_id=2541295779526181

    · 資料集中共1547張圖片，詳細內容如下：

        · 895個未配戴口罩的邊界框（0）

        · 2858個有戴口罩的邊界框（1）

        · 117個未正確配戴口罩的邊界框（2，有口罩但是配戴方法錯誤）

        · 209個配戴無防護能力口罩的邊界框（3）

        · 將VOC資料及轉換為可讀取之格式：

# 轉換VOC資料集內容
# fileList:VOC xml檔名陣列
# imgPath: 圖檔資料夾位置
# path: VOC檔案資料夾位置
# return arrayList
def convert_annotation(fileList, imgPath, path):
    resList = []
    for file in fileList:
        in_file = open(os.path.join(path,file))
        tree=ET.parse(in_file)
        root = tree.getroot()
        filename = root.find('filename').text
        data = os.path.join(imgPath,filename)

        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if int(cls) >= len(classes) or int(difficult) == 1:
                continue
            cls_id = cls
            xmlbox = obj.find('bndbox')
            b = (int(xmlbox.find('xmin').text), 
                 int(xmlbox.find('ymin').text), 
                 int(xmlbox.find('xmax').text), 
                 int(xmlbox.find('ymax').text))
            data += (" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))
        resList.append(data)
    return resList
                
# 轉換VOC資料集內容
# fileList:VOC xml檔名陣列
# imgPath: 圖檔資料夾位置
# path: VOC檔案資料夾位置
# return arrayList
def convert_annotation(fileList, imgPath, path):
    resList = []
    for file in fileList:
        in_file = open(os.path.join(path,file))
        tree=ET.parse(in_file)
        root = tree.getroot()
        filename = root.find('filename').text
        data = os.path.join(imgPath,filename)

        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if int(cls) >= len(classes) or int(difficult) == 1:
                continue
            cls_id = cls
            xmlbox = obj.find('bndbox')
            b = (int(xmlbox.find('xmin').text), 
                 int(xmlbox.find('ymin').text), 
                 int(xmlbox.find('xmax').text), 
                 int(xmlbox.find('ymax').text))
            data += (" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))
        resList.append(data)
    return resList
                
classes = ["NoMask", "GoodMask", "BadMask", "NoDefMask"]
trainPath = './day49_mask_train.txt'
if not os.path.exists(trainPath):
    list_file = open('./model_data/day49_mask_classes.txt', 'w')
    for data in classes:
        list_file.write(data)
        list_file.write('\n')
    list_file.close()

    maskPath = './data/masked_faces/voc/'
    maskImgPath = './data/masked_faces/imgs/'
    maskFileList = os.listdir(maskPath)
    
    alllist = convert_annotation(maskFileList, maskImgPath, maskPath)
    np.random.shuffle(alllist)

    list_file = open(trainPath, 'w')
    for data in alllist:
        list_file.write(data)
        list_file.write('\n')
    list_file.close()
    print('Traindata Create Over')
else:
    print("Model exist")
classes = ["NoMask", "GoodMask", "BadMask", "NoDefMask"]
trainPath = './day49_mask_train.txt'
if not os.path.exists(trainPath):
    list_file = open('./model_data/day49_mask_classes.txt', 'w')
    for data in classes:
        list_file.write(data)
        list_file.write('\n')
    list_file.close()

    maskPath = './data/masked_faces/voc/'
    maskImgPath = './data/masked_faces/imgs/'
    maskFileList = os.listdir(maskPath)
    
    alllist = convert_annotation(maskFileList, maskImgPath, maskPath)
    np.random.shuffle(alllist)

    list_file = open(trainPath, 'w')
    for data in alllist:
        list_file.write(data)
        list_file.write('\n')
    list_file.close()
    print('Traindata Create Over')
else:
    print("Model exist")

2. Model：keras-yolo3

    本次採用Model為qqwweee的keras-yolo3

annotation_path = 'day49_mask_train.txt'
log_dir = 'logs/day49_mask_local/'
classes_path = 'model_data/day49_mask_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)
input_shape = (416,416)

is_tiny_version = len(anchors) == 6
logging = TensorBoard(log_dir=log_dir)
checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)

# 分為 training 以及 validation 8:2
val_split = 0.2
with open(annotation_path) as f:
    lines = f.readlines()
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)
num_val = int(len(lines) * val_split)
num_train = len(lines) - num_val
annotation_path = 'day49_mask_train.txt'
log_dir = 'logs/day49_mask_local/'
classes_path = 'model_data/day49_mask_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)
input_shape = (416,416)

is_tiny_version = len(anchors) == 6
logging = TensorBoard(log_dir=log_dir)
checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)

# 分為 training 以及 validation 8:2
val_split = 0.2
with open(annotation_path) as f:
    lines = f.readlines()
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)
num_val = int(len(lines) * val_split)
num_train = len(lines) - num_val
    100次全layer訓練

is_tiny_version = len(anchors) == 6
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5')
model.compile(optimizer=Adam(lr=1e-3), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})

# 把所有 layer 都改為 trainable
for i in range(len(model.layers)):
    model.layers[i].trainable = True
model.compile(optimizer=Adam(lr=1e-4), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})
print('Unfreeze all of the layers.')

batch_size = 5
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_model = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=100,
        initial_epoch=0,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
model.save_weights(log_dir + 'trained_weights_final_mask_ALL.h5')
is_tiny_version = len(anchors) == 6
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5')
model.compile(optimizer=Adam(lr=1e-3), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})

# 把所有 layer 都改為 trainable
for i in range(len(model.layers)):
    model.layers[i].trainable = True
model.compile(optimizer=Adam(lr=1e-4), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})
print('Unfreeze all of the layers.')

batch_size = 5
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_model = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=100,
        initial_epoch=0,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
model.save_weights(log_dir + 'trained_weights_final_mask_ALL.h5')

在第63次就early stop

import matplotlib.pyplot as plt
%matplotlib inline
# loss 值的圖
plt.title('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(hist_model.history['loss'], color='b', label='Training Loss')
plt.plot(hist_model.history['val_loss'], color='r', label='Validation Loss')
plt.legend(loc='upper right')
import matplotlib.pyplot as plt
%matplotlib inline
# loss 值的圖
plt.title('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(hist_model.history['loss'], color='b', label='Training Loss')
plt.plot(hist_model.history['val_loss'], color='r', label='Validation Loss')
plt.legend(loc='upper right')



    50次output Layer + 50次全layer

is_tiny_version = len(anchors) == 6
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5')
model.compile(optimizer=Adam(lr=1e-3), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})

batch_size = 16
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_model = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
            steps_per_epoch=max(1, num_train//batch_size),
            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
            validation_steps=max(1, num_val//batch_size),
            epochs=50,
            initial_epoch=0,
            callbacks=[logging, checkpoint])
model.save_weights(log_dir + 'trained_weights_stage_1.h5')


# 把所有 layer 都改為 trainable
for i in range(len(model.layers)):
    model.layers[i].trainable = True
model.compile(optimizer=Adam(lr=1e-4), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})
print('Unfreeze all of the layers.')

batch_size = 5
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_mode2 = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=100,
        initial_epoch=50,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
model.save_weights(log_dir + 'trained_weights_final_mask.h5')
is_tiny_version = len(anchors) == 6
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5')
model.compile(optimizer=Adam(lr=1e-3), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})

batch_size = 16
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_model = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
            steps_per_epoch=max(1, num_train//batch_size),
            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
            validation_steps=max(1, num_val//batch_size),
            epochs=50,
            initial_epoch=0,
            callbacks=[logging, checkpoint])
model.save_weights(log_dir + 'trained_weights_stage_1.h5')


# 把所有 layer 都改為 trainable
for i in range(len(model.layers)):
    model.layers[i].trainable = True
model.compile(optimizer=Adam(lr=1e-4), 
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})
print('Unfreeze all of the layers.')

batch_size = 5
print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
hist_mode2 = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=100,
        initial_epoch=50,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
model.save_weights(log_dir + 'trained_weights_final_mask.h5')

前50次沒有設定early stop，在後32次時early stop

import matplotlib.pyplot as plt
%matplotlib inline
plt.title('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
loss = np.hstack((hist_model1.history['loss'], hist_model2.history['loss']))
val_loss = np.hstack((hist_model1.history['val_loss'], hist_model2.history['val_loss']))
plt.plot(loss, color='b', label='Training Loss')
plt.plot(val_loss, color='r', label='Validation Loss')
plt.legend(loc='upper right')
import matplotlib.pyplot as plt
%matplotlib inline
plt.title('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
loss = np.hstack((hist_model1.history['loss'], hist_model2.history['loss']))
val_loss = np.hstack((hist_model1.history['val_loss'], hist_model2.history['val_loss']))
plt.plot(loss, color='b', label='Training Loss')
plt.plot(val_loss, color='r', label='Validation Loss')
plt.legend(loc='upper right')

三、成果展示

    測試用程式碼：

image = Image.open('testimg/0004.jpg')
yolo_model = YOLO(model_path='logs/day49_mask/trained_weights_final_mask.h5', classes_path="model_data/day49_mask_classes.txt")

r_image = yolo_model.detect_image(image)
r_image
image = Image.open('testimg/0004.jpg')
yolo_model = YOLO(model_path='logs/day49_mask/trained_weights_final_mask.h5', classes_path="model_data/day49_mask_classes.txt")

r_image = yolo_model.detect_image(image)
r_image
TestImg1_100：有分辨出有口罩和無口罩


TestImg1_5050：一樣有分辨出有口罩和無口罩，信心度比100的更好




TestImg2_100：無防護功能口罩，信心指數0.79


TestImg2_5050：無防護功能口罩，信心指數0.96，成果也比100次的好




影片辨識：

    右下手語人臉有時信心指數會過低，推測為臉部過小

    左邊被裁切的人臉被分辨為無口罩




四、結論

由結果可以看出，先對outputlayer進行Training再對全Layer進行Training比直接對全Layer
Training結果還好
對圖片中臉部過小的辨識度不是很好，之後會測試將input shape及anchors大小調整試試
五、期末專題作者資訊

個人Github連結:https://github.com/GarysLin/2st-DL-CVMarathon-/tree/master/FinalProject
個人在百日馬拉松顯示名稱:Gary
