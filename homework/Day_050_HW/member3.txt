深度學習與電腦視覺期末專題_jh.su
2020/05/11
一、專題摘要
期末專題主題：使用YOLOv3模型進行浣熊與袋鼠的偵測
期末專題基本目標︰
使用YOLOv3模型訓練一個模型同時能辨識出浣熊 (raccoon) 與袋鼠 (kangaroo) 的類別與位置。
使用預訓練模型以及其在影像辨識中的作用，運用模型，辨識驗證資料集 (test data) 的效果，並透過指標判斷模型的優劣，觀察模型辨識出的結果與後續可以往哪一方面改善。
從影片檔案來讀取影像畫面，並使用預訓練模型辨識圖片，把影片 (檢視範例內為範例影片) 中的袋鼠與浣熊的位置找出來。
二、 實作方法介紹
module

b# 將 train.py 所需要的套件載入
import numpy as np
import keras.backend as K
from keras.layers import Input, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss
from yolo3.utils import get_random_data
from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper
### fix some bug?
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True    
b# 將 train.py 所需要的套件載入
import numpy as np
import keras.backend as K
from keras.layers import Input, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss
from yolo3.utils import get_random_data
from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper
### fix some bug?
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True    

data

import glob
image_files= glob.glob("final/images/*") 
len(image_files)*0.2

test_images = image_files[0:72]
train_images=  image_files[72:]
#print(train_images)
#print(test_images)
#set(test_images) | set(train_images)
with open("cvml_test_images.txt", "w") as f:
    for e in test_images: f.write(e+"\n")
with open("cvml_train_images.txt", "w") as f:
    for e in train_images: f.write(e+"\n")

with open('final/cvml_%s_images.txt'%("train")) as f:
    a = f.read().split("\n")
    #print(a)
    
train_txt = "cvml_train.txt"
#if not os.path.exists(train_txt): # 範例中訓練模型時所使用的，已經做好轉換的 annotation 檔名，增加這個檢查避免每次重新跑這段轉換的程式碼
if True:
    import xml.etree.ElementTree as ET # 載入能夠 Parser xml 文件的 library
    from os import getcwd
    sets=['train', 'test']
    classes = ["kangaroo", "raccoon"]
    
  # 把 annotation 轉換訓練時需要的資料形態
    def convert_annotation(image_fn, list_file):
        #in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))
        image_id = image_fn.replace("final/images/","" ).replace(".jpg","" )
        #print("==>",image_fn, image_id)
        in_file = open('final/labels/%s.xml'%(image_id))

        tree=ET.parse(in_file)
        root = tree.getroot()

        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if cls not in classes or int(difficult)==1:
                continue
            cls_id = classes.index(cls)
            xmlbox = obj.find('bndbox')
            b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))
            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))    
    wd = "."

    for image_set in sets:
        #image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()
        image_ids = open('final/cvml_%s_images.txt'%(image_set)).read().split("\n")       
        #annotation_path = '%s_%s.txt'%(year, image_set)
        annotation_path = "cvml_%s.txt"%(image_set)
        list_file = open(annotation_path, 'w')
        print("save annotation at %s" % annotation_path)
        for image_id in image_ids: # 只處理 100 張圖片來做範例
            #list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id))
            if image_id == "":continue
            list_file.write("./%s"%( image_id))
            convert_annotation(image_id, list_file)
            list_file.write('\n')
        list_file.close()  
 
 
 
import glob
image_files= glob.glob("final/images/*") 
len(image_files)*0.2

test_images = image_files[0:72]
train_images=  image_files[72:]
#print(train_images)
#print(test_images)
#set(test_images) | set(train_images)
with open("cvml_test_images.txt", "w") as f:
    for e in test_images: f.write(e+"\n")
with open("cvml_train_images.txt", "w") as f:
    for e in train_images: f.write(e+"\n")

with open('final/cvml_%s_images.txt'%("train")) as f:
    a = f.read().split("\n")
    #print(a)
    
train_txt = "cvml_train.txt"
#if not os.path.exists(train_txt): # 範例中訓練模型時所使用的，已經做好轉換的 annotation 檔名，增加這個檢查避免每次重新跑這段轉換的程式碼
if True:
    import xml.etree.ElementTree as ET # 載入能夠 Parser xml 文件的 library
    from os import getcwd
    sets=['train', 'test']
    classes = ["kangaroo", "raccoon"]
    
  # 把 annotation 轉換訓練時需要的資料形態
    def convert_annotation(image_fn, list_file):
        #in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))
        image_id = image_fn.replace("final/images/","" ).replace(".jpg","" )
        #print("==>",image_fn, image_id)
        in_file = open('final/labels/%s.xml'%(image_id))

        tree=ET.parse(in_file)
        root = tree.getroot()

        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if cls not in classes or int(difficult)==1:
                continue
            cls_id = classes.index(cls)
            xmlbox = obj.find('bndbox')
            b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))
            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))    
    wd = "."

    for image_set in sets:
        #image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()
        image_ids = open('final/cvml_%s_images.txt'%(image_set)).read().split("\n")       
        #annotation_path = '%s_%s.txt'%(year, image_set)
        annotation_path = "cvml_%s.txt"%(image_set)
        list_file = open(annotation_path, 'w')
        print("save annotation at %s" % annotation_path)
        for image_id in image_ids: # 只處理 100 張圖片來做範例
            #list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id))
            if image_id == "":continue
            list_file.write("./%s"%( image_id))
            convert_annotation(image_id, list_file)
            list_file.write('\n')
        list_file.close()  
 
 
 
train

annotation_path =train_txt  # 轉換好格式的標註檔案
log_dir = 'logs/2020/' # 訓練好的模型儲存的路徑
classes_path = 'model_data/cvml_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'
class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)

input_shape = (416,416) # multiple of 32, hw

is_tiny_version = len(anchors)==6 # default setting
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze

logging = TensorBoard(log_dir=log_dir)
checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)

# 分為 training 以及 validation
val_split = 0.1
with open(annotation_path) as f:
    lines = f.readlines()
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)
num_val = int(len(lines)*val_split)
num_train = len(lines) - num_val


print("lines",num_val,num_train)
# Train with frozen layers first, to get a stable loss.
# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.
# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train
if True:
    model.compile(optimizer=Adam(lr=1e-3), loss={
        # use custom yolo_loss Lambda layer.
        'yolo_loss': lambda y_true, y_pred: y_pred})

    batch_size = 8
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現
    m1= model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
            steps_per_epoch=max(1, num_train//batch_size),
            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
            validation_steps=max(1, num_val//batch_size),
            epochs=100,
            initial_epoch=0,
            callbacks=[logging, checkpoint])
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')

# Unfreeze and continue training, to fine-tune.
# Train longer if the result is not good.
if True:
    # 把所有 layer 都改為 trainable
    for i in range(len(model.layers)):
        model.layers[i].trainable = True
    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change
    print('Unfreeze all of the layers.')

    batch_size = 16 # note that more GPU memory is required after unfreezing the body
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    m2 =model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=50,
        initial_epoch=50,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
    model.save_weights(log_dir + 'trained_weights_final.h5')
annotation_path =train_txt  # 轉換好格式的標註檔案
log_dir = 'logs/2020/' # 訓練好的模型儲存的路徑
classes_path = 'model_data/cvml_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'
class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)

input_shape = (416,416) # multiple of 32, hw

is_tiny_version = len(anchors)==6 # default setting
if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes,
        freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze

logging = TensorBoard(log_dir=log_dir)
checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)

# 分為 training 以及 validation
val_split = 0.1
with open(annotation_path) as f:
    lines = f.readlines()
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)
num_val = int(len(lines)*val_split)
num_train = len(lines) - num_val


print("lines",num_val,num_train)
# Train with frozen layers first, to get a stable loss.
# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.
# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train
if True:
    model.compile(optimizer=Adam(lr=1e-3), loss={
        # use custom yolo_loss Lambda layer.
        'yolo_loss': lambda y_true, y_pred: y_pred})

    batch_size = 8
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現
    m1= model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
            steps_per_epoch=max(1, num_train//batch_size),
            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
            validation_steps=max(1, num_val//batch_size),
            epochs=100,
            initial_epoch=0,
            callbacks=[logging, checkpoint])
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')

# Unfreeze and continue training, to fine-tune.
# Train longer if the result is not good.
if True:
    # 把所有 layer 都改為 trainable
    for i in range(len(model.layers)):
        model.layers[i].trainable = True
    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change
    print('Unfreeze all of the layers.')

    batch_size = 16 # note that more GPU memory is required after unfreezing the body
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    m2 =model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
        steps_per_epoch=max(1, num_train//batch_size),
        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),
        validation_steps=max(1, num_val//batch_size),
        epochs=50,
        initial_epoch=50,
        callbacks=[logging, checkpoint, reduce_lr, early_stopping])
    model.save_weights(log_dir + 'trained_weights_final.h5')
check:

# 畫出訓練過程中 train_loss 與 val_loss 的變化
import matplotlib.pyplot as plt
%matplotlib inline

# 第一階段訓練
plt.title('Learning rate - stage 1')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(m1.history["loss"], label="train_loss")
plt.plot(m1.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()

# 第二階段訓練
plt.title('Learning rate - stage 2')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(m2.history["loss"], label="train_loss")
plt.plot(m2.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()
# 畫出訓練過程中 train_loss 與 val_loss 的變化
import matplotlib.pyplot as plt
%matplotlib inline

# 第一階段訓練
plt.title('Learning rate - stage 1')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(m1.history["loss"], label="train_loss")
plt.plot(m1.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()

# 第二階段訓練
plt.title('Learning rate - stage 2')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(m2.history["loss"], label="train_loss")
plt.plot(m2.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()


test video:

使用已經有的程式碼

https://github.com/qqwweee/keras-yolo3/blob/master/yolo_video.py

fix:

    parser.add_argument(
        '--classes_path', type=str,
        help='path to class definitions, default ' + YOLO.get_defaults("classes_path")
    )
       parser.add_argument(
        '--model_path', type=str,
        help='path to model weight file, default ' + YOLO.get_defaults("model_path")
    )
    parser.add_argument(
        '--classes_path', type=str,
        help='path to class definitions, default ' + YOLO.get_defaults("classes_path")
    )
       parser.add_argument(
        '--model_path', type=str,
        help='path to model weight file, default ' + YOLO.get_defaults("model_path")
    )


https://github.com/qqwweee/keras-yolo3/blob/master/yolo.py

    #video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))
    video_FourCC    = cv2.VideoWriter_fourcc(*"mp4v")
    #video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))
    video_FourCC    = cv2.VideoWriter_fourcc(*"mp4v")
python yolo_video.py --input=final/Kangaroo.mp4 --output=final/kangaroo.out.mp4 --classes=model_data/cvml_classes.txt --model=logs/2020/trained_weights_final.h5

python yolo_video.py --input=final/Raccoon.mp4 --output=final/Raccoon.out.mp4 --classes_path=model_data/cvml_classes.txt --model_path=logs/2020/trained_weights_final.h5

三、成果展示




我找不到 我找不到



四、結論
      總算完成了,有個機會把整的流程跑過一遍.

不過有些地方依然沒找到,或許要加點其他的資料



然後不要隨便變動環境,會省掉很多時間,本來找到個方式用tf2跑得, 但yolo用tf1,最後要新建conda env真的很煩





五、期末專題作者資訊 (請附上作者資訊)
個人Github連結︰ https://github.com/jianhaosu/2nd-DL-CVMarathon
個人在百日馬拉松顯示名稱︰jh.su