一、專題摘要 (解釋實作與說明需要解決的問題，限300~500字。)



1. 期末專題主題：浣熊與袋鼠辨識模型

2. 期末專題基本目標

    (1) 訓練一個 YOLO 模型同時能辨識出浣熊 (raccoon) 與袋鼠 (kangaroo) 的類別與位置

    (2) 使用預訓練模型來辨識測試資料集 (test data) 的效果，並透過指標判斷模型的優劣

    (3) 把範例影片中的浣熊與袋鼠的位置找出來





二、實作方法介紹 (介紹使用的程式碼、模組，並附上實作過程與結果的截圖，需圖文並茂。)



1. 使用的程式碼介紹：使用 Day41 的程式碼進行改寫，並訓練模型 (比較不一樣的地方才做說明)

    (1) 下載資料集

# 下載 raccoon 資料集
if not os.path.exists("raccoon_dataset"):
    print("raccoon_dataset doesn't exist, downloading...")
    !git clone https://github.com/experiencor/raccoon_dataset
else:
    print("raccoon_dataset exists")
    
# 下載 kangaroo 資料集
if not os.path.exists("kangaroo"):
    print("kangaroo doesn't exist, downloading...")
    !git clone https://github.com/experiencor/kangaroo
else:
    print("kangaroo exists")
# 下載 raccoon 資料集
if not os.path.exists("raccoon_dataset"):
    print("raccoon_dataset doesn't exist, downloading...")
    !git clone https://github.com/experiencor/raccoon_dataset
else:
    print("raccoon_dataset exists")
    
# 下載 kangaroo 資料集
if not os.path.exists("kangaroo"):
    print("kangaroo doesn't exist, downloading...")
    !git clone https://github.com/experiencor/kangaroo
else:
    print("kangaroo exists")


    (2) 資料前處理 (分析及整理xml資料)

 先把各自後 3% raccoon 和 kangaroo 資料集當測試資料
import numpy as np
import shutil

images_path = ['./raccoon_dataset/images/', './kangaroo/images/']  # image 路徑

for i, img in enumerate(images_path):  # img 跑 raccoon 和 kangaroo 的 images
    image_ids = os.listdir(img)  # 該 images 路徑下的檔案
    image_ids.sort()  # 檔案名稱由小排到大
    image_ids_size = len(image_ids)  # 該 images 路徑下有多少檔案
    split_num = int(np.round(image_ids_size*0.97))  # 每個資料集的後 3% 當測試集
    test_data = image_ids[split_num:]  # test 拿後 3% 筆資料
    
    for j in range(len(test_data)):
        shutil.move(img+test_data[j], 'test')
    print('Move done!')
    
    print('test_data from', img, ':', len(test_data))
    print(test_data)
    if i == 0:
        print('')
import numpy as np
import shutil

images_path = ['./raccoon_dataset/images/', './kangaroo/images/']  # image 路徑

for i, img in enumerate(images_path):  # img 跑 raccoon 和 kangaroo 的 images
    image_ids = os.listdir(img)  # 該 images 路徑下的檔案
    image_ids.sort()  # 檔案名稱由小排到大
    image_ids_size = len(image_ids)  # 該 images 路徑下有多少檔案
    split_num = int(np.round(image_ids_size*0.97))  # 每個資料集的後 3% 當測試集
    test_data = image_ids[split_num:]  # test 拿後 3% 筆資料
    
    for j in range(len(test_data)):
        shutil.move(img+test_data[j], 'test')
    print('Move done!')
    
    print('test_data from', img, ':', len(test_data))
    print(test_data)
    if i == 0:
        print('')


目標是把 raccoon 和 kangaroo 資料集轉換為訓練YOLO模型時需要的格式：image_file_path x_min,y_min,x_max,y_max,class_id


觀察浣熊與袋鼠資料集的標註資料夾名稱有點差異(浣熊的標註資料夾在annotations，而袋鼠的在annots)，所以要整理出標註檔 animals_train.txt 和 animals_val.txt
# 訓練模型時所使用的，已經做好轉換的 annotation 檔名，增加這個檢查避免每次重新跑這段轉換的程式碼
if not os.path.exists("animals_train.txt"):
    import xml.etree.ElementTree as ET  # 載入能夠 Parser xml 文件的 library
    from os import getcwd
    
    sets = ['train', 'val']  # 分為訓練集和驗證集
    classes = ["raccoon", "kangaroo"]  # raccoon(第0類) 和 kangaroo(第1類) 的資料類別
    annots_path = ['./raccoon_dataset/annotations/', './kangaroo/annots/']  # annotation 路徑
    images_path = ['./raccoon_dataset/images/', './kangaroo/images/']  # image 路徑
    
    # 把 annotation 轉換訓練時需要的資料形態
    def convert_annotation(annots_path, image_id, list_file):
        in_file = open('%s%s.xml'%(annots_path, image_id))
        tree = ET.parse(in_file)
        root = tree.getroot()
        
        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if cls not in classes or int(difficult)==1:
                continue
                
            cls_id = classes.index(cls)  # cls 分類至 cls_id(0 或 1)
            xmlbox = obj.find('bndbox')  # xmlbox 顯示影像方框(bounding box)
            b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), 
                 int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))
            
            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))
            
    for image_set in sets:  # 跑 train 和 val
        annotation_path = 'animals_%s.txt'%(image_set)
        list_file = open(annotation_path, 'w')  # 開檔+寫檔
        print("save annotation at %s" % annotation_path)
        
        for i, annots in enumerate(annots_path):  # i 跑 0 和 1，annots 跑 raccoon 和 kangaroo 的 annotation 路徑
            #print(len([name for name in os.listdir(annots) if os.path.isfile(os.path.join(annots, name))]))
            image_ids = os.listdir(annots)  # 該 annotation 路徑下的檔案
            image_ids.sort()  # 檔案名稱由小排到大
            image_ids_size = len(image_ids)  # 該 annotation 路徑下有多少檔案
            split_num = int(np.round(image_ids_size*0.8))  # 每個資料集的前 80% 當訓練集，後 20% 當驗證集
            
            if image_set == 'train':
                data = image_ids[:split_num]  # train 拿前 80% 筆資料
            else:
                data = image_ids[split_num:]  # val 拿後 20% 筆資料
                
            for image_id in data:  # 跑 xml 的 id
                xml_name = image_id.split('.')[0]  # 用'.'切割字串，然後取 .xml 前面的名稱
                list_file.write('%s%s.jpg'%(images_path[i], xml_name))  # train 和 val 各別 跑 0 和 1 的 image 路徑
                convert_annotation(annots, xml_name, list_file)  # 呼叫上面定義的函式
                list_file.write('\n')
                
        list_file.close()  # 關檔(與開檔要對應位置)
# 訓練模型時所使用的，已經做好轉換的 annotation 檔名，增加這個檢查避免每次重新跑這段轉換的程式碼
if not os.path.exists("animals_train.txt"):
    import xml.etree.ElementTree as ET  # 載入能夠 Parser xml 文件的 library
    from os import getcwd
    
    sets = ['train', 'val']  # 分為訓練集和驗證集
    classes = ["raccoon", "kangaroo"]  # raccoon(第0類) 和 kangaroo(第1類) 的資料類別
    annots_path = ['./raccoon_dataset/annotations/', './kangaroo/annots/']  # annotation 路徑
    images_path = ['./raccoon_dataset/images/', './kangaroo/images/']  # image 路徑
    
    # 把 annotation 轉換訓練時需要的資料形態
    def convert_annotation(annots_path, image_id, list_file):
        in_file = open('%s%s.xml'%(annots_path, image_id))
        tree = ET.parse(in_file)
        root = tree.getroot()
        
        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if cls not in classes or int(difficult)==1:
                continue
                
            cls_id = classes.index(cls)  # cls 分類至 cls_id(0 或 1)
            xmlbox = obj.find('bndbox')  # xmlbox 顯示影像方框(bounding box)
            b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), 
                 int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))
            
            list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))
            
    for image_set in sets:  # 跑 train 和 val
        annotation_path = 'animals_%s.txt'%(image_set)
        list_file = open(annotation_path, 'w')  # 開檔+寫檔
        print("save annotation at %s" % annotation_path)
        
        for i, annots in enumerate(annots_path):  # i 跑 0 和 1，annots 跑 raccoon 和 kangaroo 的 annotation 路徑
            #print(len([name for name in os.listdir(annots) if os.path.isfile(os.path.join(annots, name))]))
            image_ids = os.listdir(annots)  # 該 annotation 路徑下的檔案
            image_ids.sort()  # 檔案名稱由小排到大
            image_ids_size = len(image_ids)  # 該 annotation 路徑下有多少檔案
            split_num = int(np.round(image_ids_size*0.8))  # 每個資料集的前 80% 當訓練集，後 20% 當驗證集
            
            if image_set == 'train':
                data = image_ids[:split_num]  # train 拿前 80% 筆資料
            else:
                data = image_ids[split_num:]  # val 拿後 20% 筆資料
                
            for image_id in data:  # 跑 xml 的 id
                xml_name = image_id.split('.')[0]  # 用'.'切割字串，然後取 .xml 前面的名稱
                list_file.write('%s%s.jpg'%(images_path[i], xml_name))  # train 和 val 各別 跑 0 和 1 的 image 路徑
                convert_annotation(annots, xml_name, list_file)  # 呼叫上面定義的函式
                list_file.write('\n')
                
        list_file.close()  # 關檔(與開檔要對應位置)


    (3) 訓練模型

設定訓練資料的來源、模型參數，並分訓練集和驗證集的比例 (train：val = 4：1)
# 不加以下兩行會有 OSError: image file is truncated (25 bytes not processed)
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# 設定訓練資料的來源位置
annotation_path = 'animals_train.txt'  # 轉換好格式的 train 標註檔案
log_dir = 'logs/000/'  # 訓練好的模型要儲存的路徑
classes_path = 'model_data/animals_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)

# 模型參數的設定，並設定 logging, checkpoint, reduce_lr, early_stopping
input_shape = (416, 416)  # multiple of 32, hw
is_tiny_version = len(anchors)==6 # default setting

if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes, 
                              freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes, 
                         freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze

logging = TensorBoard(log_dir=log_dir)

checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', 
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.1, 
                              patience=10,  # 3
                              verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', 
                               min_delta=0.001,  # 0
                               patience=20,  # 10
                               verbose=1)

# 分為 training 以及 validation
val_split = 0.25  # train:val = 80%:20% = 4:1

with open(annotation_path) as f:
    lines = f.readlines()
    
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)

num_val = int(len(lines)*val_split)
num_train = len(lines) - num_val
# 不加以下兩行會有 OSError: image file is truncated (25 bytes not processed)
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# 設定訓練資料的來源位置
annotation_path = 'animals_train.txt'  # 轉換好格式的 train 標註檔案
log_dir = 'logs/000/'  # 訓練好的模型要儲存的路徑
classes_path = 'model_data/animals_classes.txt'
anchors_path = 'model_data/yolo_anchors.txt'

class_names = get_classes(classes_path)
num_classes = len(class_names)
anchors = get_anchors(anchors_path)

# 模型參數的設定，並設定 logging, checkpoint, reduce_lr, early_stopping
input_shape = (416, 416)  # multiple of 32, hw
is_tiny_version = len(anchors)==6 # default setting

if is_tiny_version:
    model = create_tiny_model(input_shape, anchors, num_classes, 
                              freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')
else:
    model = create_model(input_shape, anchors, num_classes, 
                         freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze

logging = TensorBoard(log_dir=log_dir)

checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', 
                             monitor='val_loss', 
                             save_weights_only=True, 
                             save_best_only=True, 
                             period=3)

reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.1, 
                              patience=10,  # 3
                              verbose=1)

early_stopping = EarlyStopping(monitor='val_loss', 
                               min_delta=0.001,  # 0
                               patience=20,  # 10
                               verbose=1)

# 分為 training 以及 validation
val_split = 0.25  # train:val = 80%:20% = 4:1

with open(annotation_path) as f:
    lines = f.readlines()
    
np.random.seed(10101)
np.random.shuffle(lines)
np.random.seed(None)

num_val = int(len(lines)*val_split)
num_train = len(lines) - num_val


第一階段訓練
# 第一階段訓練
# Train with frozen layers first, to get a stable loss.
# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.
# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train
if True:
    model.compile(optimizer=Adam(lr=1e-3),
                  loss={'yolo_loss': lambda y_true, y_pred: y_pred})  # use custom yolo_loss Lambda layer
    print('\n第一階段訓練')
    
    batch_size = 16  # 5
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    
    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現
    model_1 = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), 
                                  steps_per_epoch=max(1, num_train//batch_size), 
                                  validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), 
                                  validation_steps=max(1, num_val//batch_size), 
                                  epochs=50, 
                                  initial_epoch=0, 
                                  callbacks=[logging, checkpoint])
    
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')
# 第一階段訓練
# Train with frozen layers first, to get a stable loss.
# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.
# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train
if True:
    model.compile(optimizer=Adam(lr=1e-3),
                  loss={'yolo_loss': lambda y_true, y_pred: y_pred})  # use custom yolo_loss Lambda layer
    print('\n第一階段訓練')
    
    batch_size = 16  # 5
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    
    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現
    model_1 = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), 
                                  steps_per_epoch=max(1, num_train//batch_size), 
                                  validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), 
                                  validation_steps=max(1, num_val//batch_size), 
                                  epochs=50, 
                                  initial_epoch=0, 
                                  callbacks=[logging, checkpoint])
    
    model.save_weights(log_dir + 'trained_weights_stage_1.h5')


第二階段訓練
# 第二階段訓練
# Unfreeze and continue training, to fine-tune.
# Train longer if the result is not good.
if True:
    # 把所有 layer 都改為 trainable
    for i in range(len(model.layers)):
        model.layers[i].trainable = True
    
    model.compile(optimizer=Adam(lr=1e-4),
                  loss={'yolo_loss': lambda y_true, y_pred: y_pred})  # recompile to apply the change
    print('\n第二階段訓練: Unfreeze all of the layers.')

    batch_size = 8  # 3 # note that more GPU memory is required after unfreezing the body
    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))
    
    model_2 = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), 
                                  steps_per_epoch=max(1, num_train//batch_size), 
                                  validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), 
                                  validation_steps=max(1, num_val//batch_size), 
                                  epochs=100, 
                                  initial_epoch=50, 
                                  callbacks=[logging, checkpoint, reduce_lr, early_stopping])
    
    model.save_weights(log_dir + 'trained_weights_final.h5')


2. 使用的模組介紹
# 將 train.py 所需要的套件載入
import numpy as np
import keras.backend as K
from keras.layers import Input, Lambda
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss
from yolo3.utils import get_random_data

from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper



三、成果展示 (介紹成果的特點為何，並撰寫心得。)



1. 分析模型

# 畫出訓練過程中 train_loss 與 val_loss 的變化
import matplotlib.pyplot as plt
%matplotlib inline

# 第一階段訓練
plt.title('Learning rate - stage 1')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(model_1.history["loss"], label="train_loss")
plt.plot(model_1.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()

# 第二階段訓練
plt.title('Learning rate - stage 2')
plt.xlabel("epochs")
plt.ylabel("loss")
plt.plot(model_2.history["loss"], label="train_loss")
plt.plot(model_2.history["val_loss"], label="val_loss")
plt.legend(loc='upper right')
plt.show()



2. 測試影像

from yolo import YOLO
yolo_model = YOLO(model_path=log_dir+'trained_weights_final.h5', classes_path=classes_path)

from PIL import Image
from IPython import display

for img_file in os.listdir('test'):
    image = Image.open(os.path.join('test/', img_file))  # 讀取範例圖片
    r_image = yolo_model.detect_image(image)  # 執行 yolo 檢測，將回傳的圖片儲存在 r_image 中
    display.display(r_image)  # 顯示 r_image，可觀察到圖片上已畫上 yolov3 所檢測的 object



0.5x 好像有點低

嗨我是袋鼠

浣熊的框跨越到袋鼠家了啦

沒有偵測到貓很厲害欸

右上有一隻浣熊沒被偵測到

右下那隻不知道是什麼動物，而且還有兩個預測框

嗨我是浣熊

我在吃東西

找我有事嗎

我真受歡迎

我跳

我要睡了晚安



3. 偵測影片

參考 https://github.com/qqwweee/keras-yolo3/blob/master/yolo.py 的 detect_video 函式
偵測影片的結果不是很好：很多隻袋鼠一起出現的時候，就只有幾隻被偵測到；浣熊和狗一起出現的時候，狗會被偵測成袋鼠 (長太像?)


from yolo import YOLO
import numpy as np
import time
import cv2
from PIL import Image
import matplotlib.pyplot as plt
%matplotlib inline

log_dir = 'logs/000/' # 訓練好的模型要儲存的路徑
classes_path = 'model_data/animals_classes.txt'

yolo_model = YOLO(model_path=log_dir+'trained_weights_final.h5', classes_path=classes_path)

def detect_video(yolo, video_path, output_path=""):
    # 透過 OpenCV 擷取影像
    import cv2
    vid = cv2.VideoCapture(video_path)
    if not vid.isOpened():
        raise IOError("Couldn't open webcam or video!")
        
    # 取得影像的基本資訊
    video_FourCC = int(vid.get(cv2.CAP_PROP_FOURCC))
    video_FourCC = cv2.VideoWriter_fourcc(*'MP4V')  # 指定 video 編碼方式(mp4)
    video_fps = vid.get(cv2.CAP_PROP_FPS)  # 總共有多少 frames
    video_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),   # 每個 frame 的寬
                  int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))  # 每個 frame 的高
    
    # 設定影像的輸出
    isOutput = True if output_path != "" else False
    if isOutput:
        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)
    
    # 初始化設定
    video_cnt = 0  
    video_playtime = []  
    
    # 迭代每個 frame 來進行影像偵測
    while True:
        return_value, frame = vid.read() # 讀取每個 frame
        video_cnt += 1 
        
        # 先把每個 frame 分開偵測，再把偵測完的 frames 串接回影片，最後輸出偵測好的影片
        if return_value == True: 
            image = Image.fromarray(frame)
            start_time = time.time() 
            image = yolo.detect_image(image)  # 直接使用 yolo.py 的 detect_image 函式
            end_time = time.time()
            time_img = end_time - start_time  
            video_playtime.append(round(time_img, 3)) 
            result = np.asarray(image)
            cv2.putText(result, text='fps', org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                        fontScale=0.50, color=(255, 0, 0), thickness=2)
            if isOutput:
                out.write(result)
        else:  
            break
            
    # 釋放資源
    vid.release()  # release input video resource
    out.release()  # release output video resource
    
    return video_playtime, video_cnt



# 偵測 Kangaroo.mp4
video_playtime, video_cnt = detect_video(yolo_model, video_path='video/Kangaroo.mp4', output_path="video/Kangaroo_out.mp4")
print('\nKangaroo.mp4 total frames:', video_cnt)  # 總共有多少 frames

avg_fps = 1/np.mean(video_playtime)
print("Kangaroo.mp4 avg fps: %.3f" % avg_fps)  # 平均 fps



# 偵測 Raccoon.mp4
video_playtime, video_cnt = detect_video(yolo_model, video_path='video/Raccoon.mp4', output_path="video/Raccoon_out.mp4")
print('\nRaccoon.mp4 total frames:', video_cnt)  # 總共有多少 frames

avg_fps = 1/np.mean(video_playtime)
print("Raccoon.mp4 avg fps: %.3f" % avg_fps)  # 平均 fps




四、結論 (總結本次專題的問題與結果)



        這次專題讓我花了不少時間但學到很多，像是資料前處理、調訓練模型的參數……等，還有非常感謝很多人分享程式碼，透過程式碼我可以更了解某part的目的，也能彙整自己的想法去實現，看到成果有偵測到時真的很興奮呢！雖然有些還是沒有偵測到或偵測錯誤，可能還需要更多訓練資料吧，目前還沒有其他想法。謝謝第二屆深度學習與電腦視覺馬拉松給我機會學習。





五、期末專題作者資訊 (請附上作者資訊)



1. 個人Github連結：https://github.com/smallweii/2nd-DL-CVMarathon/blob/master/homework/Day049-050_final_project/Day049-050_final_project.ipynb

2. 個人在百日馬拉松顯示名稱：吳曉維

